{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rohitpaul09/Web-Scraping-Data-Science-Job-Listings/blob/main/Web_Scraping_Job_Listings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vWxEIoTBrXg"
   },
   "source": [
    "## **Project Name: Web Scraping Data Science Job Listings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zF7EAsW_CB0V"
   },
   "source": [
    "##### **Project Type**    - EDA\n",
    "##### **Contribution**    - Individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqleoN6vFcyO"
   },
   "source": [
    "## **Project Summary:**\n",
    "\n",
    " The primary goal is to develop an intelligent tool that streamlines data science job searches by utilizing web scraping on the Jobs website. Through the extraction of key details and the presentation of insights via visualizations, the tool aims to assist individuals in navigating the data science job market. It also keeps professionals, job seekers, and recruiters well-informed about industry trends.\n",
    "\n",
    "**Web Scraping:** The project began with web scraping job listings from the TimesJobs website. The Python code utilized the BeautifulSoup library to extract relevant details from the job listings, including job title, company name, skills required, posting time, location, and salary. The scraping process involved iterating through multiple pages of job listings, refining the extraction process, and handling diverse data structures on the website.\n",
    "\n",
    "**Data Cleaning and Transformation:** Following data extraction, the code applied techniques like strip() and replace() to organize the data systematically using the pandas library. Custom functions ensured a clean extraction of salary information, handling variations like 'Lacs,' and refining experience data for consistency.\n",
    "\n",
    "**Visualization/EDA (Exploratory Data Analysis):**\n",
    "Various visualizations were created to offer insights into different facets of the data science job market. The **WordCloud for In-Demand Skills** revealed Python, SQL, Machine Learning, and Data Mining as highly sought-after skills. **Top Cities with Most Job Openings** visualized job distribution across cities, highlighting Delhi with the highest number of openings. A **Comparison of Full-Time Jobs and Internships** indicated that 88.0% of opportunities are full-time positions, with the remaining 12.0% being internships. **Top Companies Providing Internship Opportunities** identified Maxgen Technologies as the leading provider. A **Comparison of Work from Home vs. On-Site Opportunities** showed 80.0% on-site and 20.0% work-from-home options. **Top Companies Providing Work from Home Options** showcased Minanshika Softech Solution Pvt Ltd and Soumya Gayen at the forefront. **Salary Distribution Analysis** depicted a concentration around 0-10 Lacs per annum, indicating entry-level pay scales, while **Experience Requirements Analysis** revealed a demand for beginners and 1-3 yrs experienced roles. The **Relationship Between Salary and Experience** visualized clusters, with 0-10 indicating entry-level and near 50 corresponding to higher packages for experienced professionals.\n",
    "\n",
    "In summary, the project developed a valuable tool for comprehending data science job opportunities. By extracting data from the website and employing visualizations, it offered beneficial insights for professionals, job seekers, and recruiters in the dynamic field of data science. **It is important to note that these insights are derived from a snapshot of data and may evolve with real-time updates.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yCsNfocCWdu"
   },
   "source": [
    "## **GitHub Link:** https://github.com/rohitpaul09/Web-Scraping-Data-Science-Job-Listings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YC-Zn7Fpw7In"
   },
   "source": [
    "## **Problem Statement: Navigating the Data Science Job Landscape**\n",
    "\n",
    "üöÄ Unleash your creativity in crafting a solution that taps into the heartbeat of the data science job market! Envision an ingenious project that seamlessly wields cutting-edge web scraping techniques and illuminating data analysis.\n",
    "\n",
    "üîç Your mission? To engineer a tool that effortlessly gathers job listings from a multitude of online sources, extracting pivotal nuggets such as job descriptions, qualifications, locations, and salaries.\n",
    "\n",
    "üß© However, the true puzzle lies in deciphering this trove of data. Can your solution discern patterns that spotlight the most coveted skills? Are there threads connecting job types to compensation packages? How might it predict shifts in industry demand?\n",
    "\n",
    "üéØ The core **objectives** of this challenge are as follows:\n",
    "\n",
    "1. Web Scraping Mastery: Forge an adaptable and potent web scraping mechanism. Your creation should adeptly harvest data science job postings from a diverse array of online platforms. Be ready to navigate evolving website structures and process hefty data loads.\n",
    "\n",
    "2. Data Symphony: Skillfully distill vital insights from the harvested job listings. Extract and cleanse critical information like job titles, company names, descriptions, qualifications, salaries, locations, and deadlines. Think data refinement and organization.\n",
    "\n",
    "3. Market Wizardry: Conjure up analytical tools that conjure meaningful revelations from the gathered data. Dive into the abyss of job demand trends, geographic distribution, salary variations tied to experience and location, favored qualifications, and emerging skill demands.\n",
    "\n",
    "4. Visual Magic: Weave a tapestry of visualization magic. Design captivating charts, graphs, and visual representations that paint a crystal-clear picture of the analyzed data. Make these visuals the compass that guides users through job market intricacies.\n",
    "\n",
    "üåê While the web scraping universe is yours to explore, consider these platforms as potential stomping grounds:\n",
    "\n",
    "* LinkedIn Jobs\n",
    "* Indeed\n",
    "* Naukri\n",
    "* Glassdoor\n",
    "* AngelList\n",
    "* TimesJobs\n",
    "\n",
    "\n",
    "üéà Your solution should not only decode the data science job realm but also empower professionals, job seekers, and recruiters to harness the dynamic shifts of the industry. The path is open, the challenge beckons ‚Äì are you ready to embark on this exciting journey?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGaPdi83Cov4"
   },
   "source": [
    "##**Let's Begin !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCLOT3mvwlBq"
   },
   "source": [
    "## ***1. Know Your Data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl8AsUOrvFTU"
   },
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6wt6SbX6vEn2"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0Khvn5ODJIF"
   },
   "source": [
    "###**Web Scraping Job Listings with BeautifulSoup and Pandas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PzC30ne571_-"
   },
   "outputs": [],
   "source": [
    "# Define the function to extract salary information\n",
    "def extract_salary(job_element):\n",
    "    # Extract salary information containing 'Lacs'\n",
    "    salary_tags = job_element.find_all('li')\n",
    "    for tag in salary_tags:\n",
    "        if 'Lacs' in tag.text:\n",
    "            return tag.text.strip().replace('‚ÇπRs','').replace('Lacs p.a.','')\n",
    "    return 'Not Provided'\n",
    "\n",
    "def scrape_jobs(pages):\n",
    "    all_data = []\n",
    "    experience_required_list = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        # Define the URL for each page\n",
    "        url = f'https://www.timesjobs.com/candidate/job-search.html?from=submit&luceneResultSize=25&txtKeywords=0DQT0Data%20Analyst0DQT0%20,0DQT0Data%20Mining0DQT0,0DQT0Data%20Architect0DQT0,0DQT0Machine%20Learning0DQT0,0DQT0Power%20Bi0DQT0,0DQT0Business%20Analyst0DQT0,0DQT0senior%20business%20analyst0DQT0,0DQT0Bi%20Developer0DQT0&postWeek=7&searchType=personalizedSearch&actualTxtKeywords=0DQT0Data%20Analyst0DQT0%20,0DQT0Data%20Mining0DQT0,0DQT0Data%20Architect0DQT0,0DQT0Machine%20Learning0DQT0,0DQT0Power%20Bi0DQT0,0DQT0Business%20Analyst0DQT0,senior%20business%20analyst,0DQT0Bi%20Developer0DQT0&searchBy=0&rdoOperator=OR&pDate=I&sequence={page}&startPage=1'\n",
    "        html_text = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "        # Refining the extraction process to remove unwanted strings from the experience information\n",
    "        for item in soup.find_all('ul', {'class': 'top-jd-dtl clearfix'}):\n",
    "            exp_tag = item.find('li')\n",
    "            if exp_tag and 'yrs' in exp_tag.text:\n",
    "                # Extracting the experience text and removing any unwanted strings\n",
    "                experience = exp_tag.text.replace('card_travel', '').strip().replace('yrs','')\n",
    "                experience_required_list.append(experience)\n",
    "            else:\n",
    "                experience_required_list.append('Not Mentioned')\n",
    "\n",
    "        # Extract job listings\n",
    "        job_listings = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "\n",
    "        for job in job_listings:\n",
    "            skills = job.find('span', class_='srp-skills').text.strip().replace(' ', '').replace('\\r', '').replace('\\n', '').replace('.', 'Not Provided')\n",
    "            location_element = job.find('ul', class_='top-jd-dtl clearfix')\n",
    "            location = location_element.find('span').text.strip() if location_element else 'Not Provided'\n",
    "            posted_ago = job.find('span', class_='sim-posted').span.text.strip().replace('\\r', '').replace('Posted ', '').replace('\\t', '').replace('\\n', '')\n",
    "            company_name = job.find('h3', class_='joblist-comp-name').text.strip().replace('\\r', '').replace('\\n', '').replace(' (More Jobs)', '')\n",
    "\n",
    "            # Create a dictionary for job data\n",
    "            job_data = {\n",
    "                'Job Title': job.find('h2').text.strip(),\n",
    "                'Company': company_name,\n",
    "                'Skills Required': skills,\n",
    "                'Job Posted Ago': posted_ago,\n",
    "                'Location': location,\n",
    "                'Salary(Lacs p.a.)': extract_salary(job)\n",
    "            }\n",
    "            # Append the job data to the list\n",
    "            all_data.append(job_data)\n",
    "\n",
    "    # Create a DataFrame from the collected job data\n",
    "    df = pd.DataFrame(all_data)\n",
    "    # Add the 'Experience Required' column to the DataFrame\n",
    "    df['Experience Required(Years)'] = experience_required_list\n",
    "\n",
    "    return df\n",
    "\n",
    "# Scrape the first 10 pages of job listings\n",
    "df = scrape_jobs(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DEJ1pgOvcxG"
   },
   "source": [
    "### **Dataset First View**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "wiMhqECvDVUf",
    "outputId": "aaab9ce6-1c6c-4182-a593-d012da536fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Data from Multiple Pages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills Required</th>\n",
       "      <th>Job Posted Ago</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary(Lacs p.a.)</th>\n",
       "      <th>Experience Required(Years)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Modeller</td>\n",
       "      <td>Electrobrain modern technologies pvt ltd</td>\n",
       "      <td>BusinessAnalyst,Principal,AssociateDirector,Ar...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Bengaluru / Bangalore,  Chennai,  Delhi/NCR,  ...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>4 - 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Analytics</td>\n",
       "      <td>Electrobrain modern technologies pvt ltd</td>\n",
       "      <td>powerpoint,machinelearningalgorithms,statistic...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Bengaluru / Bangalore,  Chennai,  Delhi/NCR,  ...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>4 - 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manager-Data Science</td>\n",
       "      <td>Electrobrain modern technologies pvt ltd</td>\n",
       "      <td>Projectmanagement,Finance,datascience,Automati...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Bengaluru / Bangalore,  Chennai,  Delhi/NCR,  ...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>5 - 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mining Engineer Hiring For SINGAPORE</td>\n",
       "      <td>CLOUD VISA IMMIGRATION LLP</td>\n",
       "      <td>miningengineer,miningoperator,miningengineeran...</td>\n",
       "      <td>today</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>50.00 - 95.00</td>\n",
       "      <td>3 - 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Head of Design</td>\n",
       "      <td>Rina Israni</td>\n",
       "      <td>BusinessAnalyst,assistantmanager,teamleader,te...</td>\n",
       "      <td>today</td>\n",
       "      <td>Kolkata,  Mumbai,  Noida/Greater Noida,  Pune,...</td>\n",
       "      <td>16.00 - 29.00</td>\n",
       "      <td>13 - 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Summer Internship in Ahmedabad</td>\n",
       "      <td>Maxgen Technologies</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>today</td>\n",
       "      <td>Ahmedabad,  Bhavnagar,  Gandhinagar,  Jamnagar...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>0 - 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GTU Internship in Ahmedabad</td>\n",
       "      <td>Maxgen Technologies</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Ahmedabad,  Mehsana,  Rajkot,  Surat,  Surendr...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>0 - 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Python Internship in Ahmedabad</td>\n",
       "      <td>Maxgen Technologies</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>today</td>\n",
       "      <td>Ahmedabad,  Mehsana,  Rajkot,  Surat,  Surendr...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>0 - 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Summer Internship in Pune</td>\n",
       "      <td>Maxgen Technologies</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>today</td>\n",
       "      <td>Pune,  Amravati,  Aurangabad,  Sangli,  Satara</td>\n",
       "      <td>1.00 - 2.00</td>\n",
       "      <td>0 - 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Summer Internship in Pune</td>\n",
       "      <td>Maxgen Technologies</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Pune,  Jalgaon,  Kolhapur,  Nagpur,  Solapur</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>0 - 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Job Title  \\\n",
       "0                         Data Modeller   \n",
       "1                Data Science Analytics   \n",
       "2                  Manager-Data Science   \n",
       "3  Mining Engineer Hiring For SINGAPORE   \n",
       "4                        Head of Design   \n",
       "5        Summer Internship in Ahmedabad   \n",
       "6           GTU Internship in Ahmedabad   \n",
       "7        Python Internship in Ahmedabad   \n",
       "8             Summer Internship in Pune   \n",
       "9             Summer Internship in Pune   \n",
       "\n",
       "                                        Company  \\\n",
       "0  Electrobrain modern technologies pvt ltd       \n",
       "1  Electrobrain modern technologies pvt ltd       \n",
       "2  Electrobrain modern technologies pvt ltd       \n",
       "3                CLOUD VISA IMMIGRATION LLP       \n",
       "4                               Rina Israni       \n",
       "5                       Maxgen Technologies       \n",
       "6                       Maxgen Technologies       \n",
       "7                       Maxgen Technologies       \n",
       "8                       Maxgen Technologies       \n",
       "9                       Maxgen Technologies       \n",
       "\n",
       "                                     Skills Required Job Posted Ago  \\\n",
       "0  BusinessAnalyst,Principal,AssociateDirector,Ar...      1 day ago   \n",
       "1  powerpoint,machinelearningalgorithms,statistic...      1 day ago   \n",
       "2  Projectmanagement,Finance,datascience,Automati...      1 day ago   \n",
       "3  miningengineer,miningoperator,miningengineeran...          today   \n",
       "4  BusinessAnalyst,assistantmanager,teamleader,te...          today   \n",
       "5                                       Not Provided          today   \n",
       "6                                       Not Provided      1 day ago   \n",
       "7                                       Not Provided          today   \n",
       "8                                       Not Provided          today   \n",
       "9                                       Not Provided      1 day ago   \n",
       "\n",
       "                                            Location Salary(Lacs p.a.)  \\\n",
       "0  Bengaluru / Bangalore,  Chennai,  Delhi/NCR,  ...      Not Provided   \n",
       "1  Bengaluru / Bangalore,  Chennai,  Delhi/NCR,  ...      Not Provided   \n",
       "2  Bengaluru / Bangalore,  Chennai,  Delhi/NCR,  ...      Not Provided   \n",
       "3                                          Singapore    50.00 - 95.00    \n",
       "4  Kolkata,  Mumbai,  Noida/Greater Noida,  Pune,...    16.00 - 29.00    \n",
       "5  Ahmedabad,  Bhavnagar,  Gandhinagar,  Jamnagar...      Not Provided   \n",
       "6  Ahmedabad,  Mehsana,  Rajkot,  Surat,  Surendr...      Not Provided   \n",
       "7  Ahmedabad,  Mehsana,  Rajkot,  Surat,  Surendr...      Not Provided   \n",
       "8     Pune,  Amravati,  Aurangabad,  Sangli,  Satara      1.00 - 2.00    \n",
       "9       Pune,  Jalgaon,  Kolhapur,  Nagpur,  Solapur      Not Provided   \n",
       "\n",
       "  Experience Required(Years)  \n",
       "0                     4 - 9   \n",
       "1                     4 - 9   \n",
       "2                     5 - 9   \n",
       "3                     3 - 8   \n",
       "4                   13 - 18   \n",
       "5                     0 - 1   \n",
       "6                     0 - 1   \n",
       "7                     0 - 1   \n",
       "8                     0 - 1   \n",
       "9                     0 - 1   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the head of the DataFrame with data from multiple pages\n",
    "print('Scraped Data from Multiple Pages:')\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mwbj0-xivoO-"
   },
   "source": [
    "### **Dataset Rows & Columns count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsWnmhk1oXxs",
    "outputId": "c9185553-a34d-4805-f50c-66868a645387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Data Rows Count: 250\n",
      "Scraped Data Columns Count: 7\n"
     ]
    }
   ],
   "source": [
    "# Dataset Rows & Columns count\n",
    "print('Scraped Data Rows Count:',df.shape[0])\n",
    "print('Scraped Data Columns Count:',df.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uA7strE0v5RC"
   },
   "source": [
    "### **Dataset Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bibQSWE0orOp",
    "outputId": "014a7c91-ec9a-4b8a-e18a-0da238cd0553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 7 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Job Title                   250 non-null    object\n",
      " 1   Company                     250 non-null    object\n",
      " 2   Skills Required             250 non-null    object\n",
      " 3   Job Posted Ago              250 non-null    object\n",
      " 4   Location                    250 non-null    object\n",
      " 5   Salary(Lacs p.a.)           250 non-null    object\n",
      " 6   Experience Required(Years)  250 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 13.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dataset Info\n",
    "print('Scraped Data Info:')\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2utRUVnwBxy"
   },
   "source": [
    "#### **Duplicate Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcmTD8gBo1Lw",
    "outputId": "9680e216-2fc2-49fd-d7e7-0fbdf9b3f0fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play Store Data Duplicate Value Count: 1\n"
     ]
    }
   ],
   "source": [
    "# Dataset Duplicate Value Count\n",
    "print('Play Store Data Duplicate Value Count:',len(df[df.duplicated()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDsP_6xBwLck"
   },
   "source": [
    "#### **Missing Values/Null Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlPc0TmJsnhH",
    "outputId": "b6ee1f2b-52b3-486e-c87d-36d3dcd55e5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value % in Scraped Data:\n",
      "                           datatype  not null values  null value  \\\n",
      "Job Title                    object              250           0   \n",
      "Company                      object              250           0   \n",
      "Skills Required              object              250           0   \n",
      "Job Posted Ago               object              250           0   \n",
      "Location                     object              243           7   \n",
      "Salary(Lacs p.a.)            object              250           0   \n",
      "Experience Required(Years)   object              250           0   \n",
      "\n",
      "                            null value(%)  \n",
      "Job Title                             0.0  \n",
      "Company                               0.0  \n",
      "Skills Required                       0.0  \n",
      "Job Posted Ago                        0.0  \n",
      "Location                              2.8  \n",
      "Salary(Lacs p.a.)                     0.0  \n",
      "Experience Required(Years)            0.0  \n"
     ]
    }
   ],
   "source": [
    "# Missing Values/Null Values Count\n",
    "# Function to calculate the percentage of null values in each column\n",
    "def unified_null_percent(data_fm):\n",
    "    # Convert empty strings to NaN\n",
    "    data_fm = data_fm.replace('', pd.NA)\n",
    "\n",
    "    null_info = pd.DataFrame(index=data_fm.columns)\n",
    "    null_info[\"datatype\"] = data_fm.dtypes\n",
    "    null_info[\"not null values\"] = data_fm.count()\n",
    "    null_info[\"null value\"] = data_fm.isnull().sum()\n",
    "    null_info[\"null value(%)\"] = round(data_fm.isnull().mean() * 100, 2)\n",
    "\n",
    "    return null_info\n",
    "# Display the percentage of null values for Play Store Data\n",
    "print('Null value % in Scraped Data:', unified_null_percent(df), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFbWBhjvKL4W"
   },
   "source": [
    "### What did you know about your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuMFZUyzKMuJ"
   },
   "source": [
    "The dataset is related to the online job portal industry, containing 250 rows and 7 columns obtained by scraping data from the first 10 pages of the job portal website. A 4.8% occurrence of missing values is observed in the 'Location' column. Our primary goal is to uncover trends in the current data science job market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeWK9gAnwdf5"
   },
   "source": [
    "## ***2. Understanding Your Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gbTXQT2xpLxN",
    "outputId": "db03cb30-97a3-4f42-c7d0-3c96065d28e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Dataset Columns:\n",
      "Index(['Job Title', 'Company', 'Skills Required', 'Job Posted Ago', 'Location',\n",
      "       'Salary(Lacs p.a.)', 'Experience Required(Years)'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset Columns\n",
    "print('Scraped Dataset Columns:',df.columns,sep='\\n',end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "acAl_LeIpRvB",
    "outputId": "59255a9f-c228-4d6d-be48-e13053ea94fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills Required</th>\n",
       "      <th>Job Posted Ago</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary(Lacs p.a.)</th>\n",
       "      <th>Experience Required(Years)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>214</td>\n",
       "      <td>127</td>\n",
       "      <td>229</td>\n",
       "      <td>8</td>\n",
       "      <td>66</td>\n",
       "      <td>21</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>BUSISOL SOURCING INDIA PVT. LTD</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>few days ago</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>5 - 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>57</td>\n",
       "      <td>227</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Job Title                          Company Skills Required  \\\n",
       "count                250                              250             250   \n",
       "unique               214                              127             229   \n",
       "top     Business Analyst  BUSISOL SOURCING INDIA PVT. LTD    Not Provided   \n",
       "freq                  11                               21               7   \n",
       "\n",
       "       Job Posted Ago               Location Salary(Lacs p.a.)  \\\n",
       "count             250                    250               250   \n",
       "unique              8                     66                21   \n",
       "top      few days ago  Bengaluru / Bangalore      Not Provided   \n",
       "freq               83                     57               227   \n",
       "\n",
       "       Experience Required(Years)  \n",
       "count                         250  \n",
       "unique                         56  \n",
       "top                        5 - 8   \n",
       "freq                           23  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Description\n",
    "df.describe(include='object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFcdGmIBxCrG"
   },
   "source": [
    "### **Variables Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rL_4OVcxGw9"
   },
   "source": [
    "###Descriptions for Scraped Dataset:\n",
    "**Job Title:** The specific designation associated with the job opening.\n",
    "\n",
    "**Company:** The name of the organization that has posted the job.\n",
    "\n",
    "**Skills Required:** The essential skills and qualifications needed for the job.\n",
    "\n",
    "**Job Posted Ago:** The number of days elapsed since the job was posted, providing insight into its freshness.\n",
    "\n",
    "**Location:** The list of cities where the job opportunity is available.\n",
    "\n",
    "**Salary (Lacs p.a.):** The salary range for the position on an annual basis, denoted in lakhs.\n",
    "\n",
    "**Experience Required (Years):** The number of years of professional experience required for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLaG7xnb2FEF"
   },
   "source": [
    "## 3. ***Data Wrangling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyKVbwhH2K80",
    "outputId": "9abf5f34-4167-41b5-dbef-77ac1d6f8dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Removing Duplicates:\n",
      "Scraped Dataset Rows count: 250\n",
      "Scraped Dataset Columns count: 7\n",
      "\n",
      "Shape After Removing Duplicates:\n",
      "Scraped Dataset Rows count: 249\n",
      "Scraped Dataset Columns count: 7\n"
     ]
    }
   ],
   "source": [
    "# Show Dataset Rows & Columns count Before Removing Duplicates\n",
    "print('Shape Before Removing Duplicates:')\n",
    "print('Scraped Dataset Rows count:',df.shape[0])\n",
    "print('Scraped Dataset Columns count:',df.shape[1],end='\\n\\n')\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Show Dataset Rows & Columns count After Removing Duplicates\n",
    "print('Shape After Removing Duplicates:')\n",
    "print('Scraped Dataset Rows count:',df.shape[0])\n",
    "print('Scraped Dataset Columns count:',df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0xt9dUK3SQz",
    "outputId": "83cc1d27-38af-4cf1-b75f-a6855f26dfd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before Removing Missing Values:\n",
      "Scraped Dataset Rows count: 249\n",
      "Scraped Dataset Columns count: 7\n",
      "\n",
      "Shape After Removing Missing Values:\n",
      "Scraped Dataset Rows count: 242\n",
      "Scraped Dataset Columns count: 7\n"
     ]
    }
   ],
   "source": [
    "# Show Dataset Rows & Columns count Before Removing Missing Values\n",
    "print('Shape Before Removing Missing Values:')\n",
    "print('Scraped Dataset Rows count:',df.shape[0])\n",
    "print('Scraped Dataset Columns count:',df.shape[1],end='\\n\\n')\n",
    "\n",
    "# Replace empty strings with NaN in the 'Location' column\n",
    "df['Location'].replace('', pd.NA, inplace=True)\n",
    "\n",
    "# Drop rows with null values in the 'Location' column\n",
    "df.dropna(subset=['Location'], inplace=True)\n",
    "\n",
    "# Show Dataset Rows & Columns count After Removing Missing Values\n",
    "print('Shape After Removing Missing Values:')\n",
    "print('Scraped Dataset Rows count:',df.shape[0])\n",
    "print('Scraped Dataset Columns count:',df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7M54N9y34I8",
    "outputId": "872a004a-6861-411a-cf1c-c4f1c9dbb599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated number of missing values in Scraped Dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Job Title                     0\n",
       "Company                       0\n",
       "Skills Required               0\n",
       "Job Posted Ago                0\n",
       "Location                      0\n",
       "Salary(Lacs p.a.)             0\n",
       "Experience Required(Years)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values again to confirm\n",
    "print('Updated number of missing values in Scraped Dataset:')\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhvsmgiyUNQO"
   },
   "source": [
    "### What all manipulations have you done and insights you found?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te4PBUUsUJo5"
   },
   "source": [
    "- Show Dataset Rows & Columns count Before Removing Duplicates\n",
    "  - Rows count: 250\n",
    "  - Columns count: 7\n",
    "- Remove duplicates\n",
    "  - `df.drop_duplicates(inplace=True)`\n",
    "- Show Dataset Rows & Columns count After Removing Duplicates\n",
    "  - Rows count: 248\n",
    "  - Columns count: 7\n",
    "  \n",
    "- Show Dataset Rows & Columns count Before Removing Missing Values\n",
    "  - Rows count: 248\n",
    "  - Columns count: 7\n",
    "\n",
    "- Replace empty strings with NaN in the 'Location' column\n",
    "  - `df['Location'].replace('', pd.NA, inplace=True)`\n",
    "  \n",
    "- Drop rows with null values in the 'Location' column\n",
    "  - `df.dropna(subset=['Location'], inplace=True)`\n",
    "\n",
    "- Show Dataset Rows & Columns count After Removing Missing Values\n",
    "  - Rows count: 236\n",
    "  - Columns count: 7\n",
    "\n",
    "- Check missing values again to confirm\n",
    "  - `df.isnull().sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x242b2b60430>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['database_name']\n",
    "collection = db['job_data']\n",
    "df_dict = df.to_dict(orient='records')\n",
    "collection.insert_many(df_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Define the function to extract salary information\n",
    "def extract_salary(job_element):\n",
    "    # Extract salary information containing 'Lacs'\n",
    "    salary_tags = job_element.find_all('li')\n",
    "    for tag in salary_tags:\n",
    "        if 'Lacs' in tag.text:\n",
    "            return tag.text.strip().replace('‚ÇπRs','').replace('Lacs p.a.','')\n",
    "    return 'Not Provided'\n",
    "\n",
    "def scrape_jobs(pages):\n",
    "    all_data = []\n",
    "    experience_required_list = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        # Define the URL for each page\n",
    "        url = f'https://www.timesjobs.com/candidate/job-search.html?from=submit&luceneResultSize=25&txtKeywords=0DQT0Data%20Analyst0DQT0%20,0DQT0Data%20Mining0DQT0,0DQT0Data%20Architect0DQT0,0DQT0Machine%20Learning0DQT0,0DQT0Power%20Bi0DQT0,0DQT0Business%20Analyst0DQT0,0DQT0senior%20business%20analyst0DQT0,0DQT0Bi%20Developer0DQT0&postWeek=7&searchType=personalizedSearch&actualTxtKeywords=0DQT0Data%20Analyst0DQT0%20,0DQT0Data%20Mining0DQT0,0DQT0Data%20Architect0DQT0,0DQT0Machine%20Learning0DQT0,0DQT0Power%20Bi0DQT0,0DQT0Business%20Analyst0DQT0,senior%20business%20analyst,0DQT0Bi%20Developer0DQT0&searchBy=0&rdoOperator=OR&pDate=I&sequence={page}&startPage=1'\n",
    "        html_text = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "        # Refining the extraction process to remove unwanted strings from the experience information\n",
    "        for item in soup.find_all('ul', {'class': 'top-jd-dtl clearfix'}):\n",
    "            exp_tag = item.find('li')\n",
    "            if exp_tag and 'yrs' in exp_tag.text:\n",
    "                # Extracting the experience text and removing any unwanted strings\n",
    "                experience = exp_tag.text.replace('card_travel', '').strip().replace('yrs','')\n",
    "                experience_required_list.append(experience)\n",
    "            else:\n",
    "                experience_required_list.append('Not Mentioned')\n",
    "\n",
    "        # Extract job listings\n",
    "        job_listings = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "\n",
    "        for job in job_listings:\n",
    "            skills = job.find('span', class_='srp-skills').text.strip().replace(' ', '').replace('\\r', '').replace('\\n', '').replace('.', 'Not Provided')\n",
    "            location_element = job.find('ul', class_='top-jd-dtl clearfix')\n",
    "            location = location_element.find('span').text.strip() if location_element else 'Not Provided'\n",
    "            posted_ago = job.find('span', class_='sim-posted').span.text.strip().replace('\\r', '').replace('Posted ', '').replace('\\t', '').replace('\\n', '')\n",
    "            company_name = job.find('h3', class_='joblist-comp-name').text.strip().replace('\\r', '').replace('\\n', '').replace(' (More Jobs)', '')\n",
    "\n",
    "            # Create a dictionary for job data\n",
    "            job_data = {\n",
    "                'Job Title': job.find('h2').text.strip(),\n",
    "                'Company': company_name,\n",
    "                'Skills Required': skills,\n",
    "                'Job Posted Ago': posted_ago,\n",
    "                'Location': location,\n",
    "                'Salary(Lacs p.a.)': extract_salary(job)\n",
    "            }\n",
    "            # Append the job data to the list\n",
    "            all_data.append(job_data)\n",
    "\n",
    "    # Create a DataFrame from the collected job data\n",
    "    df = pd.DataFrame(all_data)\n",
    "    # Add the 'Experience Required' column to the DataFrame\n",
    "    df['Experience Required(Years)'] = experience_required_list\n",
    "\n",
    "    return df\n",
    "\n",
    "# Scrape the first 10 pages of job listings\n",
    "df = scrape_jobs(10)\n",
    "\n",
    "# Display the head of the DataFrame with data from multiple pages\n",
    "print('Scraped Data from Multiple Pages:')\n",
    "print(df.head(10))\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['job_database']\n",
    "collection = db['job_collection']\n",
    "\n",
    "# Convert DataFrame to dictionary\n",
    "df_dict = df.to_dict(orient='records')\n",
    "\n",
    "# Insert the data into MongoDB\n",
    "collection.insert_many(df_dict)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
